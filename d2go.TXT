import torch , torchvision
import detectron2 , d2go
from detectron2.data.datasets import register_coco_instances
from detectron2.data import MetadataCatalog ,DatasetCatalog
from d2go.runner import Detectron2GoRunner
from d2go.model_zoo import model_zoo
from d2go.runner import Detectron2GoRunner
from d2go.model_zoo import model_zoo
from detectron2.utils.visualizer import ColorMode
from detectron2.utils.visualizer import Visualizer
from detectron2.utils.video_visualizer import VideoVisualizer
from detectron2.engine import DefaultPredictor
import cv2
import time
import numpy as np

import os

#register coco instances:

register_coco_instances("my_dataset_train", {}, "/content/drive/MyDrive/d2go/output.json", "/content/seperateimages")
register_coco_instances("my_dataset_val", {}, "/content/drive/MyDrive/d2go/output.json", "/content/seperateimages")


sample_metadata = MetadataCatalog.get("my_dataset_train")
dataset_dicts = DatasetCatalog.get("my_dataset_train")

def prepare_for_launch():
    runner = Detectron2GoRunner()
    cfg = runner.get_default_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("mask_rcnn_fbnetv3g_fpn.yaml"))
    cfg.MODEL_EMA.ENABLED = False
    cfg.DATASETS.TRAIN = ("my_dataset_train",)
    cfg.DATASETS.TEST = ("my_dataset_val",)
    cfg.DATALOADER.NUM_WORKERS = 4 #no of cpu to be used
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("mask_rcnn_fbnetv3g_fpn.yaml")  # Let training initialize from model zoo
    cfg.SOLVER.IMS_PER_BATCH = 2
    cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR
    cfg.SOLVER.MAX_ITER = 1000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset
    cfg.SOLVER.STEPS = []        # do not decay learning rate
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set the testing threshold for this model
    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
    # NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.
    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
    return cfg, runner

cfg, runner = prepare_for_launch()
model = runner.build_model(cfg)
runner.do_train(cfg, model, resume=True)


cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "/content/output/model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set the testing threshold for this model
cfg.DATASETS.TEST = ("my_dataset_train", )
predictor = DefaultPredictor(cfg)

#cap = cv2.VideoCapture(0)
# cap = cv2.VideoCapture(1)
cap = cv2.VideoCapture('https://www.youtube.com/watch?v=Hf4qbtzxc-Q')

cam_h = cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)

cam_w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)
cam_h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)
fps = cap.get(cv2.CAP_PROP_FPS)

print('Capture width:{}'.format(cam_w))
print('Capture height:{}'.format(cam_h))
print('Capture fps:{}'.format(fps))

win_name = 'JAL'
cv2.namedWindow(win_name, cv2.WINDOW_NORMAL)

viz = VideoVisualizer(sample_metadata, instance_mode=ColorMode.IMAGE_BW)
# fourcc = cv2.VideoWriter_fourcc(*'X264')
fourcc = cv2.VideoWriter_fourcc(*'MP4V')
# fourcc = cv2.VideoWriter_fourcc(*'MJPG')
vw = cv2.VideoWriter('out.mp4', fourcc, 5, (int(cam_w), int(cam_h)))

if cap.isOpened():

    inference_time_cma = 0
    drawing_time_cma = 0
    n = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # tic = time.time()
        res = predictor(frame)
        # toc = time.time()

        # curr_inference_time = toc - tic
        # inference_time_cma = (n * inference_time_cma + curr_inference_time) / (n+1)


        # print('cma inference time: {:0.3} sec'.format(inference_time_cma))

        # tic2 = time.time()

        drawned_frame = frame.copy() # make a copy of the original frame
        
        # draw on the frame with the res
        # v = Visualizer(drawned_frame[:, :, ::-1],
        #             metadata=plastic_metadata, 
        #             scale=0.8, 
        #             instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels
        # )
        v_out = viz.draw_instance_predictions(drawned_frame, res["instances"].to("cpu"))
        # v_out = viz.draw_instance_predictions(drawned_frame[:, :, ::-1], res["instances"].to("cpu"))
        drawned_frame = v_out.get_image()
        
        cv2.imshow(win_name, drawned_frame)
        # toc2 = time.time()
        vw.write(drawned_frame)

        # curr_drawing_time = toc2 - tic2
        # drawing_time_cma = (n * drawing_time_cma + curr_drawing_time) / (n+1)
        
        # print('cma draw time: {:0.3} sec'.format(drawing_time_cma))

        if cv2.waitKey(1) & 0xff == ord('q'):
            break

        n += 1

vw.release()
cap.release()
print('Done.')